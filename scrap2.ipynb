{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kylesullivan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kylesullivan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHARACTER JSONS: []\n",
      "\n",
      "GET OR CREATE BASE FACTS PLAYER DESCRIPTION: An quirky contestant that is must see TV on a reality show. True\n",
      "RESPONSE: Name='Luna Willows' Age=27 Likes=['DIY crafts', 'plant collecting', 'hiking', 'improv theater', 'creating quirky outfits'] Dislikes=['routines', 'conflict', 'silence', 'negativity'] Occupation='Reality Show Contestant' Home_City='Portland, Oregon'\n",
      "Generated facts: {'Name': 'Luna Willows', 'Age': 27, 'Likes': ['DIY crafts', 'plant collecting', 'hiking', 'improv theater', 'creating quirky outfits'], 'Dislikes': ['routines', 'conflict', 'silence', 'negativity'], 'Occupation': 'Reality Show Contestant', 'Home_City': 'Portland, Oregon'}\n",
      "PERSONA: Luna Willows, a 27-year-old Reality Show Contestant. You are passionate about DIY crafts, plant collecting, hiking. You have aversions to routines, conflict, silence. Some key facts about yourself are: home_city, Portland, Oregon. Your overall game strategy is Backstabbing, reflecting your judgement which *tends* to be Partial, cooperation which *tends* to be Unyielding, outlook which *tends* to be Realistic, initiative which *tends* to be Dominant, generosity which *tends* to be Moderate, social which *tends* to be Assertive, mind which *tends* to be Analytical, openness which *tends* to be Moderately, and stress which *tends* to be Relaxed. Luna Willows speaks in the style of a 27 year old Reality Show Contestant from no place in particular. They have a balanced outlook on life. They're generally balanced in their stress levels.Use this information to guide how they speak, in terms of tone, word choice, sentence structure, phrasing, terseness, verbosity, and overall demeanor.\n",
      "GROUP: D\n",
      "CHARACTER: <text_adventure_games.things.characters.GenerativeAgent object at 0x1131087a0>\n",
      "Character Luna Willows starts at Boardroom and belongs to Group D\n",
      "\n",
      "Overwriting log file is data...\n",
      "The game data will be overwritten when you run `game.save_simulation_data()`\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import openai\n",
    "sys.path.insert(0, \"../\")\n",
    "sys.path.insert(0, \"../..\")\n",
    "from GenAgentsBoardroom.text_adventure_games.parsing import GptParser3\n",
    "from GenAgentsBoardroom.test.game_setup import build_boardroom\n",
    "\n",
    "game = build_boardroom(\n",
    "    experiment_name=\"boardroom-test\",\n",
    "    experiment_id=1,\n",
    "    num_characters=1,\n",
    "    max_ticks=2,\n",
    "    personas_path=\"game_personas\",\n",
    "    # random_placement=True,\n",
    ")\n",
    "\n",
    "parser = GptParser3(game, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objects': ['game', 'goal'],\n",
       " 'other_named_entities': [],\n",
       " 'misc_deps': [],\n",
       " 'characters': []}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.extract_keywords(\"What have I been doing to achieve the goal of the game?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages for lemmatization and singularization.\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from inflect import engine\n",
    "import spacy\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the lemmatizer and inflect engine.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "inflect_engine = engine()\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to standardize words by converting to lowercase, lemmatizing, and singularizing.\n",
    "def standardize_word(word):\n",
    "    word = word.lower()\n",
    "    # Preserve hyphenated words\n",
    "    if '-' in word:\n",
    "        return word\n",
    "    # Check if the word is a punctuation\n",
    "    if word in string.punctuation or word in [\"the\", \"a\", \"an\"]:\n",
    "        return None\n",
    "    # Try lemmatizing with different parts of speech\n",
    "    for pos in [wordnet.VERB, wordnet.NOUN, wordnet.ADJ, wordnet.ADV]:\n",
    "        lemmatized_word = lemmatizer.lemmatize(word, pos)\n",
    "        if lemmatized_word != word:  # If lemmatization changed the word\n",
    "            word = lemmatized_word\n",
    "            if pos == wordnet.NOUN:\n",
    "                word = inflect_engine.singular_noun(word) or word\n",
    "                break\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def extract_keywords(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    custom_stopwords = {\n",
    "        \"he\",\n",
    "        \"it\",\n",
    "        \"i\",\n",
    "        \"you\",\n",
    "        \"she\",\n",
    "        \"they\",\n",
    "        \"we\",\n",
    "        \"us\",\n",
    "        \"'s\",\n",
    "        \"this\",\n",
    "        \"that\",\n",
    "        \"these\",\n",
    "        \"those\",\n",
    "        \"them\",\n",
    "    }\n",
    "\n",
    "    doc = nlp(text)\n",
    "    keys = defaultdict(set)\n",
    "    for w in doc:\n",
    "        if w.text.lower() in custom_stopwords:\n",
    "            continue\n",
    "\n",
    "        if w.pos_ in [\"PROPN\"]:\n",
    "            # compounds = [j for j in w.children if j.dep_ == \"compound\"]\n",
    "            # if compounds:\n",
    "            #     continue\n",
    "            compound_parts = []\n",
    "            for child in w.subtree:\n",
    "                if child.text.lower() not in custom_stopwords:\n",
    "                    if '-' in child.text:\n",
    "                        compound_parts.append(child.text.lower())\n",
    "                    else:\n",
    "                        compound_parts.append(\n",
    "                            \" \".join(\n",
    "                                [\n",
    "                                    standardize_word(word)\n",
    "                                    for word in child.text.split()\n",
    "                                    if standardize_word(word)\n",
    "                                ]\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "            compound_noun = \" \".join(compound_parts)\n",
    "\n",
    "            exists, name = False, None # self.check_if_character_exists(compound_noun)\n",
    "            if exists:\n",
    "                keys[\"characters\"].add(name.lower())\n",
    "            else:\n",
    "                keys[\"misc_deps\"].add(\n",
    "                    \" \".join(\n",
    "                        [\n",
    "                            standardize_word(word)\n",
    "                            for word in compound_noun.split()\n",
    "                            if standardize_word(word)\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # Process each word in the compound noun separately\n",
    "            for part in compound_parts:\n",
    "                exists, name = False, None # self.check_if_character_exists(part)\n",
    "                if exists:\n",
    "                    keys[\"characters\"].add(name.lower())\n",
    "                else:\n",
    "                    keys[\"misc_deps\"].add(part)\n",
    "            continue\n",
    "\n",
    "        if \"subj\" in w.dep_:\n",
    "            exists, name = False, None  # self.check_if_character_exists(w.text)\n",
    "            if exists:\n",
    "                keys[\"characters\"].add(name.lower())\n",
    "            else:\n",
    "                keys[\"misc_deps\"].add(\n",
    "                    \" \".join(\n",
    "                        [\n",
    "                            standardize_word(word)\n",
    "                            for word in w.text.split()\n",
    "                            if standardize_word(word)\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "        if \"obj\" in w.dep_:\n",
    "            exists, name = False, None  # self.check_if_character_exists(w.text)\n",
    "            if exists:\n",
    "                keys[\"characters\"].add(name.lower())\n",
    "            else:\n",
    "                keys[\"objects\"].add(\n",
    "                    \" \".join(\n",
    "                        [\n",
    "                            standardize_word(word)\n",
    "                            for word in w.text.split()\n",
    "                            if standardize_word(word)\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in [\"PERSON\", \"ORG\", \"GPE\"]:\n",
    "            exists, name = False, None  # self.check_if_character_exists(ent.text)\n",
    "            if exists:\n",
    "                keys[\"characters\"].add(name.lower())\n",
    "            else:\n",
    "                keys[\"misc_deps\"].add(\n",
    "                    \" \".join(\n",
    "                        [\n",
    "                            standardize_word(word)\n",
    "                            for word in ent.text.split()\n",
    "                            if standardize_word(word)\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    keys = {k: list(v) for k, v in keys.items()}\n",
    "\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_phrases_from_text(text):\n",
    "    # # Replace all punctuation with spaces\n",
    "    # text = text.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
    "    text = text.replace(\"-\", \" \").replace(\"â€“\", \" \")\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Collect all subtrees\n",
    "    subtrees = []\n",
    "    for token in doc:\n",
    "        subtrees.append(list(token.subtree))\n",
    "        print(\"SUBTREE:\", [token.text for token in subtrees[-1]])\n",
    "\n",
    "    # Initialize sets to collect noun, verb, proper noun phrases, named entities, objects, and subjects across all subtrees\n",
    "    all_noun_phrases = set()  # Use a set to avoid duplicates\n",
    "    all_verb_phrases = set()  # Use a set to avoid duplicates\n",
    "    all_proper_nouns = set()  # Use a set to avoid duplicates\n",
    "    all_named_entities = set()  # Set to collect named entities\n",
    "    all_objects = set()  # Set to collect objects\n",
    "    all_subjects = set()  # Set to collect subjects\n",
    "\n",
    "    # Process each subtree\n",
    "    for subtree in subtrees:\n",
    "        # print(\"SUBTREE:\", [token.text for token in subtree])\n",
    "        for token in subtree:\n",
    "            if token.pos_ == \"NOUN\":\n",
    "                # Include the noun itself without adjectives\n",
    "                if token.text.lower() not in [\"the\", \"a\", \"an\"]:  # Remove articles\n",
    "                    all_noun_phrases.add(token.text)\n",
    "\n",
    "                # Include noun phrases with adjectives\n",
    "                noun_phrase = []\n",
    "                for child in token.lefts:\n",
    "                    if child.pos_ == \"ADJ\":\n",
    "                        noun_phrase.append(child.text)\n",
    "                if token.text.lower() not in [\"the\", \"a\", \"an\"]:  # Remove articles\n",
    "                    noun_phrase.append(token.text)\n",
    "                all_noun_phrases.add(\" \".join(noun_phrase))\n",
    "\n",
    "                # Check for compound nouns\n",
    "                compound_noun = \" \".join([child.text for child in token.subtree if child.dep_ in [\"compound\", \"amod\"]] + [token.text])\n",
    "                if compound_noun != token.text and compound_noun.lower() not in [\"the\", \"a\", \"an\"]:  # Remove articles\n",
    "                    all_noun_phrases.add(compound_noun)\n",
    "\n",
    "                # Add to objects set\n",
    "                if token.text.lower() not in [\"the\", \"a\", \"an\"]:  # Remove articles\n",
    "                    all_objects.add(token.text)\n",
    "\n",
    "            if token.pos_ == \"VERB\" and token.text.lower() not in [\"go\", \"do\", \"try\", \"make\", \"take\", \"have\", \"get\", \"put\", \"come\", \"see\", \"say\"]:  # Exclude simple verbs\n",
    "                # Include the verb itself without adverbs\n",
    "                all_verb_phrases.add(token.text)\n",
    "\n",
    "            if token.pos_ == \"PROPN\":\n",
    "                # Include the proper noun itself\n",
    "                all_proper_nouns.add(token.text)\n",
    "\n",
    "                # Include contiguous proper nouns\n",
    "                proper_noun_phrase = []\n",
    "                for child in token.lefts:\n",
    "                    if list(child.rights) or list(child.lefts):\n",
    "                        print(\"CHILD:\", child.text)\n",
    "                        print(\"CHILDREN:\", list(child.lefts), list(child.rights))\n",
    "                    if child.pos_ == \"PROPN\":\n",
    "                        proper_noun_phrase.append(child.text)\n",
    "                proper_noun_phrase.append(token.text)\n",
    "                all_proper_nouns.add(\" \".join(proper_noun_phrase))\n",
    "\n",
    "            if \"subj\" in token.dep_:  # Check if the token is a subject\n",
    "                all_subjects.add(token.text)\n",
    "\n",
    "    # Extract named entities from the document\n",
    "    for ent in doc.ents:\n",
    "        # Remove articles from the entity text\n",
    "        entity_text = ent.text.split()\n",
    "        entity_text = [word for word in entity_text if word.lower() not in [\"the\", \"a\", \"an\"]]\n",
    "        cleaned_entity_text = \" \".join(entity_text)\n",
    "        if cleaned_entity_text:  # Ensure it's not empty after removing articles\n",
    "            all_named_entities.add(cleaned_entity_text)\n",
    "\n",
    "    return (\n",
    "        all_noun_phrases,\n",
    "        all_verb_phrases,\n",
    "        all_named_entities,\n",
    "        all_objects,\n",
    "        all_subjects,\n",
    "        # all_proper_nouns,\n",
    "    )  # Return the objects and subjects set as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noun_phrases, verb_phrases, named_entities, objects, subjects, = extract_phrases_from_text(\n",
    "#     \"Engage Fiona and Jasper Jensen in light-hearted philosophical discussions to build rapport.\"\n",
    "#     \"Then go to the Apple Store to buy a high-tech new Macbook Pro. Mark Zuckerberg is the CEO of Facebook.\"\n",
    "#     \"My goodness, these new computers are so expensive!\"\n",
    "#     \"What shall I do?\"\n",
    "# )\n",
    "\n",
    "# phrase_names = [\"noun_phrases\", \"verb_phrases\", \"named_entities\", \"objects\", \"subjects\", \"proper_nouns\"]\n",
    "# for phrase_name, phrases in zip(phrase_names, [noun_phrases, verb_phrases, named_entities, objects, subjects]):\n",
    "#     print(phrase_name)\n",
    "#     print(phrases)\n",
    "#     print('-'*200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_text = \"Engage with Fiona and Jasper Jensen in light-hearted philosophical discussions to build rapport. Then go to the Apple Store to buy a high-tech new Macbook Pro. Mark Zuckerberg is the CEO of Facebook.\"\n",
    "# # original_text = (\n",
    "# #     \"Casey Johnson looked around at the surroundings at the camp and saw Luna Blake.\"\n",
    "# # )\n",
    "# noun_phrases, verb_phrases, named_entities, objects, subjects = extract_phrases_from_text(original_text)\n",
    "\n",
    "# for text in [*noun_phrases, *verb_phrases, *named_entities, *objects, *subjects]:\n",
    "#     print(text)\n",
    "#     print(parser.extract_keywords(text))\n",
    "#     print()\n",
    "\n",
    "# print('-'*200)\n",
    "# print(parser.extract_keywords(original_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'misc_deps': ['',\n",
       "  'engage fiona',\n",
       "  'hearted',\n",
       "  'engage',\n",
       "  'light',\n",
       "  'tech',\n",
       "  'facebook',\n",
       "  'build',\n",
       "  'in',\n",
       "  'to',\n",
       "  'mark',\n",
       "  'fiona',\n",
       "  'store',\n",
       "  'macbook pro',\n",
       "  'jasper jensen',\n",
       "  'macbook',\n",
       "  'zuckerberg',\n",
       "  'high',\n",
       "  'pro',\n",
       "  'new',\n",
       "  '-',\n",
       "  'and',\n",
       "  'apple',\n",
       "  'jasper',\n",
       "  'rapport',\n",
       "  'high - tech new macbook pro mark zuckerberg',\n",
       "  'jensen',\n",
       "  'high - tech new macbook pro',\n",
       "  'apple store',\n",
       "  'mark zuckerberg',\n",
       "  'discussion',\n",
       "  'engage fiona and jasper jensen in light - hearted philosophical discussion to build rapport',\n",
       "  'philosophical'],\n",
       " 'objects': ['dog', 'rapport', 'discussion']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_keywords(\n",
    "    \"Engage Fiona and Jasper Jensen in light-hearted philosophical discussions to build rapport. Then go to the Apple Store to buy a high-tech new Macbook Pro. Mark Zuckerberg is the CEO of Facebook. Don't forget the hot dogs!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'misc_deps': ['hearted',\n",
       "  'engage',\n",
       "  'light',\n",
       "  'tech',\n",
       "  'facebook',\n",
       "  'build',\n",
       "  'mark',\n",
       "  'fiona',\n",
       "  'engage fiona jasper jensen light hearted philosophical discussion build rapport',\n",
       "  'store',\n",
       "  'jasper jensen',\n",
       "  'macbook',\n",
       "  'zuckerberg',\n",
       "  'high',\n",
       "  'pro',\n",
       "  'apple',\n",
       "  'jasper',\n",
       "  'rapport',\n",
       "  'jensen',\n",
       "  'apple store',\n",
       "  'high tech macbook pro mark zuckerberg',\n",
       "  'discussion',\n",
       "  'high tech macbook pro',\n",
       "  'philosophical'],\n",
       " 'objects': ['dog', 'rapport', 'discussion'],\n",
       " 'other_named_entities': ['engage fiona', 'macbook pro', 'mark zuckerberg'],\n",
       " 'characters': []}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.extract_keywords(\n",
    "    \"Engage Fiona and Jasper Jensen in light-hearted philosophical discussions to build rapport. Then go to the Apple Store to buy a high-tech new Macbook Pro. Mark Zuckerberg is the CEO of Facebook. Don't forget the hot dogs!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'misc_deps': ['casey johnson',\n",
       "  'luna blake',\n",
       "  'blake',\n",
       "  'johnson',\n",
       "  'luna',\n",
       "  'casey'],\n",
       " 'objects': ['camp', 'surroundings'],\n",
       " 'other_named_entities': [],\n",
       " 'characters': []}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.extract_keywords(\n",
    "    \"Casey Johnson looked around at the surroundings at the camp and saw Luna Blake.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'misc_deps': ['casey johnson',\n",
       "  'petunia',\n",
       "  'lola petunia',\n",
       "  'johnson',\n",
       "  'casey',\n",
       "  'lola'],\n",
       " 'objects': ['winner', 'jury', 'vote', 'game'],\n",
       " 'other_named_entities': [],\n",
       " 'characters': []}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.extract_keywords(\n",
    "    \"Casey Johnson survived the vote. Casey Johnson received 1 out of 3 votes. Lola Petunia was exiled from the game but now sits on the final jury, where they will be allowed to cast a vote to help determine the game winner.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'misc_deps': ['casey johnson',\n",
       "  'petunia',\n",
       "  'lola petunia',\n",
       "  'johnson',\n",
       "  'casey',\n",
       "  'lola'],\n",
       " 'objects': ['winner', 'game', 'vote', 'jury'],\n",
       " 'other_named_entities': [],\n",
       " 'characters': []}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.extract_keywords(\n",
    "    \"Casey Johnson survived the vote. Casey Johnson received 1 out of 3 votes. Lola Petunia was exiled from the game but now sits on the final jury, where they will be allowed to cast a vote to help determine the game winner.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is an example:\n",
    "\n",
    "# Input:\n",
    "# \"Engage Fiona and Jasper in light-hearted philosophical discussions to build rapport.\"\n",
    "\n",
    "# Output:\n",
    "# {'misc_deps': ['philosophical',\n",
    "#   'light',\n",
    "#   'hearted',\n",
    "#   'fiona',\n",
    "#   'engage fiona jasper in light hearted philosophical discussions to build rapport',\n",
    "#   'engage',\n",
    "#   'jasper',\n",
    "#   'discussion',\n",
    "#   'build',\n",
    "#   'to',\n",
    "#   'rapport',\n",
    "#   'in'],\n",
    "#  'objects': ['discussion', 'rapport'],\n",
    "#  'other_named_entities': ['engage fiona'],\n",
    "#  'characters': []}\n",
    "\n",
    "# I need to avoid including adjectives (e.g., 'light-hearted') and I need to avoid these long segments (e.g., 'engage fiona jasper in light hearted philosophical discussions to build rapport'). Only compound words should be kept together (they should also be processed separately)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'misc_deps': ['hearted',\n",
       "  'engage',\n",
       "  '-',\n",
       "  'to',\n",
       "  'discussion',\n",
       "  'fiona',\n",
       "  'rapport',\n",
       "  'build',\n",
       "  'in',\n",
       "  'light',\n",
       "  'philosophical',\n",
       "  'engage fiona jasper in light - hearted philosophical discussion to build rapport'],\n",
       " 'characters': ['jasper quinlan'],\n",
       " 'objects': ['discussion', 'rapport'],\n",
       " 'other_named_entities': ['engage fiona']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.extract_keywords(\n",
    "    \"Engage Fiona and Jasper in light-hearted philosophical discussions to build rapport.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'misc_deps': ['philosophical',\n",
       "  'light',\n",
       "  'hearted',\n",
       "  'fiona',\n",
       "  'engage fiona jasper in light hearted philosophical discussions to build rapport',\n",
       "  'engage',\n",
       "  'jasper',\n",
       "  'discussion',\n",
       "  'build',\n",
       "  'to',\n",
       "  'rapport',\n",
       "  'in'],\n",
       " 'objects': ['discussion', 'rapport'],\n",
       " 'other_named_entities': ['engage fiona'],\n",
       " 'characters': []}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.extract_keywords(\n",
    "    \"Engage Fiona and Jasper in light-hearted philosophical discussions to build rapport.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(self, text, actions=False):\n",
    "    \"\"\"\n",
    "    Extracts keywords from the provided text, identifying characters, objects, and optionally actions.\n",
    "\n",
    "    This method processes the input text using natural language processing to identify and categorize keywords, such\n",
    "    as characters, objects, and actions, while filtering out common stopwords. It returns a dictionary containing sets of\n",
    "    identified characters, objects, actions, and miscellaneous dependencies based on the text analysis.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text from which to extract keywords.\n",
    "        actions (bool, optional): Whether to extract actions from the text. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with keys for \"characters\", \"objects\", \"actions\", \"misc_deps\", and \"other_named_entities\",\n",
    "                each containing a list of identified keywords.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the input text is empty; if so, return None.\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    # Define a set of custom stopwords to filter out common, non-informative words, including basic verbs.\n",
    "    custom_stopwords = {\n",
    "        \"a\",\n",
    "        \"an\",\n",
    "        \"and\",\n",
    "        \"he\",\n",
    "        \"it\",\n",
    "        \"i\",\n",
    "        \"you\",\n",
    "        \"she\",\n",
    "        \"they\",\n",
    "        \"we\",\n",
    "        \"us\",\n",
    "        \"'s\",\n",
    "        \"this\",\n",
    "        \"that\",\n",
    "        \"these\",\n",
    "        \"those\",\n",
    "        \"them\",\n",
    "        \"their\",\n",
    "        \"my\",\n",
    "        \"your\",\n",
    "        \"our\",\n",
    "        \"the\",\n",
    "        \"is\",\n",
    "        \"are\",\n",
    "        \"was\",\n",
    "        \"were\",\n",
    "        \"be\",\n",
    "        \"being\",\n",
    "        \"been\",\n",
    "        \"have\",\n",
    "        \"has\",\n",
    "        \"had\",\n",
    "        \"do\",\n",
    "        \"does\",\n",
    "        \"did\",\n",
    "        \"say\",\n",
    "        \"says\",\n",
    "        \"said\",\n",
    "        \"go\",\n",
    "        \"goes\",\n",
    "        \"went\",\n",
    "        \"make\",\n",
    "        \"makes\",\n",
    "        \"made\",\n",
    "        \"know\",\n",
    "        \"knows\",\n",
    "        \"knew\",\n",
    "        \"think\",\n",
    "        \"thinks\",\n",
    "        \"thought\",\n",
    "        \"take\",\n",
    "        \"takes\",\n",
    "        \"took\",\n",
    "        \"see\",\n",
    "        \"sees\",\n",
    "        \"saw\",\n",
    "        \"come\",\n",
    "        \"comes\",\n",
    "        \"came\",\n",
    "        \"want\",\n",
    "        \"wants\",\n",
    "        \"wanted\",\n",
    "        \"like\",\n",
    "        \"likes\",\n",
    "        \"liked\",\n",
    "    }\n",
    "\n",
    "    # Import necessary packages for lemmatization and singularization.\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.corpus import wordnet\n",
    "    from inflect import engine\n",
    "\n",
    "    # Initialize the lemmatizer and inflect engine.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    inflect_engine = engine()\n",
    "\n",
    "    # Function to standardize words by converting to lowercase, lemmatizing, and singularizing.\n",
    "    def standardize_word(word):\n",
    "        word = word.lower()\n",
    "        # Try lemmatizing with different parts of speech\n",
    "        for pos in [wordnet.VERB, wordnet.NOUN, wordnet.ADJ, wordnet.ADV]:\n",
    "            lemmatized_word = lemmatizer.lemmatize(word, pos)\n",
    "            if lemmatized_word != word:  # If lemmatization changed the word\n",
    "                word = lemmatized_word\n",
    "                if pos == wordnet.NOUN:\n",
    "                    word = inflect_engine.singular_noun(word) or word\n",
    "                    break\n",
    "            return word\n",
    "\n",
    "    # Process the input text using the natural language processing model.\n",
    "    doc = self.nlp(text)\n",
    "\n",
    "    # Initialize a defaultdict to store identified keywords categorized by type.\n",
    "    keys = defaultdict(set)\n",
    "\n",
    "    # Iterate over each word in the processed document.\n",
    "    for w in doc:\n",
    "        # Skip the word if it is in the custom stopwords set.\n",
    "        if w.text.lower() in custom_stopwords:\n",
    "            continue\n",
    "\n",
    "        # Check if the word is a proper noun (PROPN).\n",
    "        if w.pos_ in [\"PROPN\"]:\n",
    "            # If the proper noun has compound words, handle the entire compound noun.\n",
    "            compound_noun = \" \".join(\n",
    "                [\n",
    "                    child.text\n",
    "                    for child in w.subtree\n",
    "                    if child.text.lower() not in custom_stopwords\n",
    "                ]\n",
    "            ).lower()\n",
    "            exists, name = self.check_if_character_exists(compound_noun)\n",
    "            if exists:\n",
    "                # If the character exists, add it to the characters set.\n",
    "                keys[\"characters\"].add(name.lower())\n",
    "            else:\n",
    "                # If not, add the compound noun to miscellaneous dependencies.\n",
    "                keys[\"misc_deps\"].add(standardize_word(compound_noun))\n",
    "\n",
    "            # Process each word in the compound noun separately.\n",
    "            for part in compound_noun.split():\n",
    "                exists, name = self.check_if_character_exists(part)\n",
    "                if exists:\n",
    "                    # If the character exists, add it to the characters set.\n",
    "                    keys[\"characters\"].add(name.lower())\n",
    "                else:\n",
    "                    # If not, add the word to miscellaneous dependencies.\n",
    "                    keys[\"misc_deps\"].add(standardize_word(part))\n",
    "            continue\n",
    "\n",
    "        # Check if the word is a subject in the dependency parse.\n",
    "        if \"subj\" in w.dep_:\n",
    "            exists, name = self.check_if_character_exists(w.text.lower())\n",
    "            if exists:\n",
    "                # If the character exists, add it to the characters set.\n",
    "                keys[\"characters\"].add(name.lower())\n",
    "            else:\n",
    "                # If not, add the word to miscellaneous dependencies.\n",
    "                keys[\"misc_deps\"].add(standardize_word(w.text))\n",
    "\n",
    "        # Check if the word is an object in the dependency parse.\n",
    "        if \"obj\" in w.dep_:\n",
    "            exists, name = self.check_if_character_exists(w.text.lower())\n",
    "            if exists:\n",
    "                # If the character exists, add it to the characters set.\n",
    "                keys[\"characters\"].add(name.lower())\n",
    "            else:\n",
    "                # If not, add the word to the objects set.\n",
    "                keys[\"objects\"].add(standardize_word(w.text))\n",
    "\n",
    "        if actions:\n",
    "            # Check if the word is a verb (action).\n",
    "            if w.pos_ == \"VERB\":\n",
    "                keys[\"actions\"].add(standardize_word(w.text))\n",
    "\n",
    "    # Iterate over named entities in the document.\n",
    "    for ent in doc.ents:\n",
    "        # Check if the entity is a person, organization, or geopolitical entity.\n",
    "        if ent.label_ in [\"PERSON\", \"ORG\", \"GPE\"]:\n",
    "            exists, name = self.check_if_character_exists(ent.text.lower())\n",
    "            if exists:\n",
    "                # If the character exists, add it to the characters set.\n",
    "                keys[\"characters\"].add(name.lower())\n",
    "            else:\n",
    "                cleaned_entity = \" \".join(\n",
    "                    [\n",
    "                        standardize_word(w.text)\n",
    "                        for word in ent.text.split()\n",
    "                        if word.lower() not in custom_stopwords\n",
    "                    ]\n",
    "                ).lower()\n",
    "                keys[\"other_named_entities\"].add(cleaned_entity)\n",
    "\n",
    "    # Remove duplicates between 'misc_deps' and 'other_named_entities'\n",
    "    keys[\"other_named_entities\"] = keys[\"other_named_entities\"] - keys[\"misc_deps\"]\n",
    "\n",
    "    # Remove duplicates between 'characters' and 'other_named_entities'\n",
    "    keys[\"other_named_entities\"] = keys[\"other_named_entities\"] - keys[\"characters\"]\n",
    "\n",
    "    # Remove duplicates between 'objects' and 'other_named_entities'\n",
    "    keys[\"other_named_entities\"] = keys[\"other_named_entities\"] - keys[\"objects\"]\n",
    "\n",
    "    # Convert the sets in the keys dictionary to lists for easier handling.\n",
    "    keys = {k: list(v) for k, v in keys.items()}\n",
    "\n",
    "    # Return the dictionary containing categorized keywords.\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
